<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-87339102-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-87339102-1');
	</script>

	<title>Kristina Monakhova</title>
	<meta name="description" content="Kristina Monakhova's Homepage">
	<meta name="author" content="Kristina Monakhova">

	<link href="minimal.css" rel="stylesheet">
</head>
<body>
<div class="container">
	<header>
		<a class="logo" href="index.html"> Kristina Monakhova</a>
		<nav class="float-right">
			<ul>
				<li> <a href = "mailto:monakhova@berkeley.edu"> email </a> / <a href="resources/Monakhova_CV.pdf">CV</a> / <a href="https://scholar.google.com/citations?user=X71o1ykAAAAJ&hl">google scholar</a></li>
			</ul>
		</nav>
	</header>

	<div class="row">
			<img class="float-left" src="resources/new_headshot1_circle.png" style="height:170px;">
			<p> Hello! I am an <a href="http://www.eecs.berkeley.edu">EECS</a> PhD Candidate at UC Berkeley advised by <a href="http://www.laurawaller.com/">Laura Waller</a>.
				I am interested computational imaging, which is the joint design of imaging hardware and algorithms. My work is at the intesection of signal processing,
				optics, optimization, and machine learning.
			</p>
		  <p> I completed my B.S. in Electrical Engineering from the <a href="http://www.buffalo.edu">State University of New York at Buffalo</a> in May 2016.
			  At Buffalo, I was involved in a nanosatellite mission and several other space-related research projects, which you can read more about on my old website <a href="http://monakhova.weebly.com">here.</a>
		  </p>



	    </p>
  </div>

<hr>
	<div class="row">
		<div class="col-2">
		<h4>News:</h4>
	</div>
	<div class="col-8">
	Oct. 2020 - Selected to participate in <a href="https://eecs.berkeley.edu/rising-stars-2020">EECS Rising Stars 2020 Workshop</a>  <br>
		Jul. 2020 - Selected to participate in the <a href="https://nextprofnexus.engin.umich.edu/">NextProf Nexus 2020 Workshop</a>
</div>
  </div>

<hr>
	<h2>Research</h2>
	<div class="row">
			<h3>Spectral DiffuserCam: lensless snapshot hyperspectral imaging</h3>
			<img class="float-right" src="resources/spectralDiffuser.jpg" style="width:250px;">
			<p><b>K. Monkhova</b>*, K. Yanny*, N. Aggarwal, L. Waller <br>

			<a href="https://waller-lab.github.io/SpectralDiffuserCam/">Project Page</a> /
			<a href="https://www.youtube.com/watch?v=ReH0x_W3glM&feature=emb_title">Video</a> /
			<a href="https://github.com/Waller-Lab/SpectralDiffuserCam">Code</a> /
			<a href = "http://www.osapublishing.org/optica/abstract.cfm?URI=optica-7-10-1298">Paper (Optica)</a>
		</p>

		<!--	<img class="float-right" src="resources/ladmm.png" style="width:400px;"> -->


			<p>In this work, we propose a novel, <b>compact, and inexpensive computational camera for snapshot hyperspectral imaging</b>. Our system consists of a repeated spectral filter array placed directly on the image sensor and a diffuser placed close to the sensor. Each point in the world maps to a unique pseudorandom pattern on the spectral filter array, which encodes multiplexed spatio-spectral information. A sparsity-constrained inverse problem solver then recovers the hyperspectral volume with good spatio-spectral resolution. By using a spectral filter array, our hyperspectral imaging framework is flexible and can be designed with contiguous or non-contiguous spectral filters that can be chosen for a given application. </p>

	</div>

	<div class="row">
			<h3>Miniscope3D: optimized single-shot miniature 3D fluorescence microscopy</h3>

			<p>K. Yanny, N. Antipa, W. Liberti, S. Dehaeck, <b>K. Monakhova</b>, F. L. Liu, K. Shen, R. Ng, L. Waller  <br>

				<img class="float-right" src="resources/bear.gif" style="width:500px;">
			<a href = "https://www.nature.com/articles/s41377-020-00403-7">Paper (Nature LS&A)</a>
		</p>

		<!--	<img class="float-right" src="resources/ladmm.png" style="width:400px;"> -->


			<p>

				In this work, we replace the tube lens of a <a href="http://miniscope.org/index.php/Main_Page">Miniscope</a> with an engineered
				and optimized diffuser that's printed using a <a href="https://www.nanoscribe.com/en/">Nanoscribe</a> 3D printer. The resulting imager
				is inexpensive, tiny (the size of a quarter), and can capture <b>3D fluorescent volumes from a single image</b>, with resulting 3 micron lateral resolution
				and 10 micron axial resolution at video rates with no moving parts. Check out more of our 3D videos of water bear videos
				 <a href="https://www.nature.com/articles/s41377-020-00403-7#Sec20">here</a>.
	</div>

	<div class="row">
			<h3>Learning for Lensless Imaging</h3>
			<img class="float-right" src="resources/video_dataset_square.gif" style="width:250px;">
		<p>
			<b>K. Monakhova</b>, J. Yurtsever, G. Kuo, N. Antipa, K. Yanny, L. Waller <br>
			<a href="https://waller-lab.github.io/LenslessLearning/index.html">Project Page</a> /
			<a href = "https://www.osapublishing.org/oe/abstract.cfm?uri=oe-27-20-28075">Paper (Optics Express)</a>
		</p>

		<!--	<img class="float-right" src="resources/ladmm.png" style="width:400px;"> -->


			<p>Mask-based lensless imagers, like <a = href="https://waller-lab.github.io/DiffuserCam/">DiffuserCam</a>, can be small, compact, and capture higher-dimensional
				information (3D, temporal), but the reconstruction time is slow and the
				image quality is often degraded.  In this work, we show that we can use
				knowledge of optical system physics along with deep learning to form an unrolled
			model-based network to solve the reconstruction problem, thereby using <b>physics
				 + deep learning</b> together to speed up and improve image reconstructions.
				 As compared to traditional methods, our architecture achieves better perceptual
				 image quality and runs 20× faster, enabling interactive previewing of the scene.
			 </p>

<!--
 		Papers:
			<p> <b>Kristina Monakhova</b>, Joshua Yurtsever, Grace Kuo, Nick Antipa, Kyrollos
				Yanny, and Laura Waller, "Learned reconstructions for practical mask-based
				lensless imaging," Opt. Express 27, 28075-28090 (2019)
				<a href = "https://www.osapublishing.org/oe/abstract.cfm?uri=oe-27-20-28075"> [pdf] </a>
			</p>

			<p>
			<p> <b>Kristina Monakhova</b>, Nick Antipa, and Laura Waller, “Learning for lensless mask-based imaging,”
				in Computational Optical Sensing and Imaging, pp. CTu3A–2, Optical Society of America, 2019 <a href = "https://www.osapublishing.org/abstract.cfm?uri=COSI-2019-CTu3A.2">[pdf] </a> </p>
-->
  </div>

<hr>
<div class="row">
	<div class="col-6">
		<h4>Work Experience</h4>
		<ul>
			<li>MIT Lincoln Laboratory, Summer 2016</li>
			<li>Northrop Grumman Electronic Systems, Winter 2016</li>
			<li>Northrop Grumman Aerospace Systems, Summer 2015</li>
			<li>Carnegie Mellon Robotics Institute, Summer 2014</li>
			<li>NASA Marshall Robotics Academy, Summer 2013 </li>
		</ul>
	</div>
	<div class="col-6">
		<h4>Awards and Recognition</h4>
		<ul>
			<li>UC Berkeley EECS Chairs’ Graduate Award, 2020 </li>
			<li>NSF Graduate Research Fellowship, 2016</li>
			<li>NDSEG Graduate Research Fellowship, 2016 (declined)</li>
			<li>Barry M. Goldwater Scholarship, 2015 </li>
			<li>University at Buffalo Presidential Scholar, 4 year full ride scholarship</li>
		</ul>
	</div>
</div>

<!--<div class="row"><div class="col-12"><img class="center" src="resources/img3.jpg" style="width:1000px;"></div></div>
-->
	<footer>
			&copy; Kristina Monakhova <!-- Link not required, but appreciated. --><a class="float-right" href="http://minimalcss.com">Minimal</a>
	</footer><!-- footer -->
</div>
</body>
</html>
